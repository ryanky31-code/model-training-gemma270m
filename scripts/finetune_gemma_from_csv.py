"""Fine-tune Gemma (270M) from a CSV of synthetic samples.

This script adapts the Google Colab notebook in `site/en/gemma/docs/core/huggingface_text_full_finetune.ipynb`
to train on a CSV dataset like the synthetic_wifi_5ghz_outdoor.csv generated by the user's generator.

Usage (example):
  python scripts/finetune_gemma_from_csv.py \
    --csv ./synthetic_wifi_5ghz_outdoor.csv \
    --base-model "google/gemma-3-270m-it" \
    --checkpoint-dir ./gemma_finetune_checkpoint \
    --target-field recommended_channel_mhz \
    --num-epochs 3 \
    --per-device-batch-size 4

Notes:
- This performs supervised fine-tuning (SFT) using TRL's SFTTrainer. It formats each CSV row as a short
  text prompt (user) and a short target answer (assistant). That works well for predicting discrete labels
  such as recommended_channel_mhz or simple numeric outputs such as expected_throughput_mbps.
- You need to accept the Gemma license on Hugging Face and have an HF token with read/write access.
"""
import argparse
import os
import math
import json

def build_prompt_from_row(row, include_many_fields=True):
    # Build a compact human-readable prompt from numeric/structured fields
    parts = []
    parts.append(f"Scenario id: {row.get('scenario_id')}")
    parts.append(f"Device A coords: {row.get('device_a_coordinates')}")
    parts.append(f"Device B coords: {row.get('device_b_coordinates')}")
    parts.append(f"Distance (m): {row.get('link_distance_m'):.1f}")
    parts.append(f"Environment: {row.get('environment_type')}, Density: {row.get('area_density')}")
    parts.append(f"Fresnel clear %: {row.get('fresnel_clear_pct')}")
    parts.append(f"Weather: {row.get('weather_condition')}, temp C: {row.get('weather_temp_c'):.1f}")
    parts.append(f"Noise floor (dBm): {row.get('noise_floor_dbm'):.1f}, RSSI (dBm): {row.get('rssi_dbm'):.1f}")
    if include_many_fields and 'channel_bandwidth_mhz' in row:
        parts.append(f"Channel BW (MHz): {int(row.get('channel_bandwidth_mhz'))}")
    # Final user question
    parts.append("\nBased on the above scenario, answer the question precisely:")
    return "\n".join(parts)

def build_target_from_row(row, target_field):
    # Create assistant response depending on desired target
    if target_field == 'recommended_channel_mhz':
        return str(int(row.get('recommended_channel_mhz')))
    elif target_field == 'expected_throughput_mbps':
        return f"{float(row.get('expected_throughput_mbps')):.2f}"
    else:
        # Default: return a small JSON with channel and throughput
        out = {
            'recommended_channel_mhz': int(row.get('recommended_channel_mhz')),
            'expected_throughput_mbps': float(row.get('expected_throughput_mbps'))
        }
        return json.dumps(out)


def prepare_dataset(csv_path, target_field, test_size=0.1, max_rows=None):
    # Local imports to avoid requiring heavy libraries when only parsing args/help
    import pandas as pd
    from datasets import Dataset

    df = pd.read_csv(csv_path)
    if max_rows is not None:
        df = df.head(max_rows)

    # Create a conversational style dataset compatible with the Gemma SFT example
    samples = []
    for _, r in df.iterrows():
        prompt = build_prompt_from_row(r)
        target = build_target_from_row(r, target_field)
        messages = [
            {"role": "user", "content": prompt + "\n\nQuestion: Provide only the answer (no explanation)."},
            {"role": "assistant", "content": target}
        ]
        samples.append({"messages": messages})

    ds = Dataset.from_pandas(pd.DataFrame(samples))
    if test_size and 0.0 < test_size < 1.0:
        ds = ds.train_test_split(test_size=test_size)
    else:
        ds = {'train': ds}
    return ds


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--csv', required=True, help='Path to CSV dataset')
    parser.add_argument('--base-model', default='google/gemma-3-270m-it', help='HF model id')
    parser.add_argument('--checkpoint-dir', default='./gemma_finetune', help='Output/checkpoint directory')
    parser.add_argument('--num-epochs', type=int, default=3)
    parser.add_argument('--per-device-batch-size', type=int, default=4)
    parser.add_argument('--learning-rate', type=float, default=5e-5)
    parser.add_argument('--target-field', default='recommended_channel_mhz', help='target field to train on')
    parser.add_argument('--max-rows', type=int, default=None, help='Limit rows (useful for quick smoke tests)')
    args = parser.parse_args()

    # Prepare data
    print(f"Loading CSV from {args.csv} and preparing dataset (target={args.target_field})...")
    ds = prepare_dataset(args.csv, args.target_field, test_size=0.1, max_rows=args.max_rows)

    # Lazy import to avoid requiring heavy libs if only using data prep
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from trl import SFTConfig, SFTTrainer

    # Load model and tokenizer
    print(f"Loading base model {args.base_model} (this requires that you accepted the license on HF)...")
    model = AutoModelForCausalLM.from_pretrained(
        args.base_model,
        torch_dtype='auto',
        device_map='auto',
        attn_implementation='eager'
    )
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)

    torch_dtype = model.dtype

    sft_args = SFTConfig(
        output_dir=args.checkpoint_dir,
        max_length=512,
        packing=False,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.per_device_batch_size,
        gradient_checkpointing=False,
        optim='adamw_torch_fused',
        logging_steps=1,
        save_strategy='epoch',
        eval_strategy='epoch',
        learning_rate=args.learning_rate,
        fp16=True if torch_dtype == torch.float16 else False,
        bf16=True if torch_dtype == torch.bfloat16 else False,
        lr_scheduler_type='constant',
        push_to_hub=False,
        report_to='tensorboard',
        dataset_kwargs={
            'add_special_tokens': False,
            'append_concat_token': True,
        }
    )

    print('Creating SFTTrainer...')
    trainer = SFTTrainer(
        model=model,
        args=sft_args,
        train_dataset=ds['train'],
        eval_dataset=ds['test'] if 'test' in ds else None,
        processing_class=tokenizer,
    )

    print('Starting training...')
    trainer.train()
    trainer.save_model()
    print(f"Training complete. Checkpoints saved to {args.checkpoint_dir}")


if __name__ == '__main__':
    main()
