{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b68a26b",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ryanky31-code/model-training-gemma270m/blob/main/site/en/gemma/docs/core/huggingface_text_full_finetune_with_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e24bc",
   "metadata": {},
   "source": [
    "##### Copyright 2025 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f276b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f96b0",
   "metadata": {},
   "source": [
    "# Full Model Fine-Tune using Hugging Face Transformers (with embedded dataset generator)\n",
    "\n",
    "This notebook is based on the working example `huggingface_text_full_finetune.ipynb` but embeds a synthetic dataset generator and adapts the data-loading cells so you can run end-to-end in Colab or locally.\n",
    "\n",
    "Sections:\n",
    "- Install dependencies\n",
    "- Generate synthetic dataset (small default for smoke tests)\n",
    "- Save dataset and compute SHA256\n",
    "- Convert CSV â†’ conversational dataset (train/test)\n",
    "- Load model & tokenizer\n",
    "- Configure SFT and train\n",
    "- Test inference\n",
    "\n",
    "Defaults are set to small values so you can test quickly; increase sample counts and epochs for real training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab friendly)\n",
    "# Uncomment in Colab or run in your environment\n",
    "# %pip install torch transformers datasets trl accelerate tensorboard sentencepiece protobuf pandas numpy\n",
    "\n",
    "print('If running in Colab: enable the above pip install cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c66d1",
   "metadata": {},
   "source": [
    "## 1) Imports and reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import random\n",
    "import hashlib\n",
    "import os\n",
    "import zipfile\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reproducibility\n",
    "RSEED = 42\n",
    "random.seed(RSEED)\n",
    "np.random.seed(RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f5c77",
   "metadata": {},
   "source": [
    "## 2) Dataset generator (embedded)\n",
    "\n",
    "This is the user's generator adapted to run inside the notebook. Change `N_SAMPLES` for bigger sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (adjust for smoke tests or full runs)\n",
    "N_SAMPLES = 200  # small default for smoke tests; increase to 10000+ for real training\n",
    "OUT_DIR = '/content' if os.path.exists('/content') else '.'\n",
    "CSV_PATH = os.path.join(OUT_DIR, 'synthetic_wifi_5ghz_outdoor.csv')\n",
    "ZIP_PATH = os.path.join(OUT_DIR, 'synthetic_wifi_5ghz_outdoor.zip')\n",
    "\n",
    "# Constants\n",
    "FREQS_MHZ = list(range(4900, 6101, 5))\n",
    "ENVIRONMENTS = [\"Urban\", \"Rural\"]\n",
    "DENSITIES    = [\"Low\", \"Medium\", \"High\"]\n",
    "WEATHER_COND = [\"Clear\", \"Cloudy\", \"Rain\", \"Fog\", \"Snow\", \"Storm\"]\n",
    "OBST_TYPES   = [\"None\", \"Tree\", \"Building\", \"Vehicle\", \"Crane\", \"Billboard\"]\n",
    "TX_ANT_GAIN_DB = 15.0\n",
    "RX_ANT_GAIN_DB = 15.0\n",
    "BANDWIDTHS_MHZ = [20, 40, 80, 160]\n",
    "\n",
    "# Helper functions\n",
    "def haversine_m(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000.0\n",
    "    p = math.pi / 180.0\n",
    "    dlat = (lat2 - lat1) * p\n",
    "    dlon = (lon2 - lon1) * p\n",
    "    a = (math.sin(dlat / 2) ** 2 + math.cos(lat1 * p) * math.cos(lat2 * p) * math.sin(dlon / 2) ** 2)\n",
    "    return 2 * R * math.asin(math.sqrt(a))\n",
    "\n",
    "def snr_to_efficiency_bps_per_hz(snr_db: float) -> float:\n",
    "    return 10.0 / (1.0 + math.exp(-(snr_db - 25.0) / 4.0))\n",
    "\n",
    "def fspl_db(distance_m: float, freq_mhz: float) -> float:\n",
    "    if distance_m < 1:\n",
    "        distance_m = 1.0\n",
    "    d_km = distance_m / 1000.0\n",
    "    return 32.45 + 20 * math.log10(d_km) + 20 * math.log10(freq_mhz)\n",
    "\n",
    "def weather_extra_loss_db(weather: str, distance_m: float) -> float:\n",
    "    d_km = distance_m / 1000.0\n",
    "    base = {\"Clear\": 0.0, \"Cloudy\": 0.2, \"Fog\": 0.6, \"Rain\": 0.8, \"Snow\": 0.9, \"Storm\": 1.5}[weather]\n",
    "    return base * d_km\n",
    "\n",
    "def density_obstruction_factor(env: str, density: str) -> float:\n",
    "    return {\"Urban\": {\"Low\": 0.5, \"Medium\": 1.5, \"High\": 3.0},\n",
    "            \"Rural\": {\"Low\": 0.1, \"Medium\": 0.4, \"High\": 0.8}}[env][density]\n",
    "\n",
    "def fresnel_penalty_db(f_clear: float) -> float:\n",
    "    if f_clear >= 90: return 0.0\n",
    "    if f_clear >= 70: return 1.0\n",
    "    if f_clear >= 50: return 3.0\n",
    "    if f_clear >= 30: return 6.0\n",
    "    return 10.0\n",
    "\n",
    "def obstruction_penalty_db(obstructed: bool, obst_type: str) -> float:\n",
    "    if not obstructed or obst_type == \"None\": return 0.0\n",
    "    return {\"Tree\": 3.0, \"Vehicle\": 2.0, \"Billboard\": 4.0, \"Building\": 8.0, \"Crane\": 5.0}.get(obst_type, 2.0)\n",
    "\n",
    "\n",
    "def generate_synthetic_row(scenario_id: int):\n",
    "    environment = random.choice(ENVIRONMENTS)\n",
    "    density = random.choice(DENSITIES)\n",
    "    lat_a = random.uniform(33.0, 36.0)\n",
    "    lon_a = random.uniform(35.0, 37.0)\n",
    "    el_a = random.uniform(5, 900)\n",
    "    lat_b = lat_a + random.uniform(-0.15, 0.15)\n",
    "    lon_b = lon_a + random.uniform(-0.15, 0.15)\n",
    "    el_b = random.uniform(5, 900)\n",
    "    distance_m = haversine_m(lat_a, lon_a, lat_b, lon_b)\n",
    "    weather = random.choice(WEATHER_COND)\n",
    "    humidity = random.uniform(20, 95)\n",
    "    temp_c = random.uniform(-5, 42)\n",
    "\n",
    "    f_range = {\"Urban\": {\"Low\": (60, 100), \"Medium\": (40, 90), \"High\": (15, 80)},\n",
    "               \"Rural\": {\"Low\": (85, 100), \"Medium\": (65, 100), \"High\": (45, 100)}}\n",
    "    fresnel_clear = random.uniform(*f_range[environment][density])\n",
    "\n",
    "    obstructed = random.random() < (0.65 if (environment == \"Urban\" and density == \"High\") else 0.35 if environment == \"Urban\" else 0.2 if density != \"Low\" else 0.1)\n",
    "    obst_type = random.choice(OBST_TYPES if obstructed else [\"None\"])\n",
    "\n",
    "    nf_range = {\"Urban\": {\"Low\": (-105, -92), \"Medium\": (-100, -88), \"High\": (-95, -82)},\n",
    "                \"Rural\": {\"Low\": (-115, -102), \"Medium\": (-110, -98), \"High\": (-108, -96)}}\n",
    "    noise_floor_dbm = random.uniform(*nf_range[environment][density])\n",
    "    noise_dbm = noise_floor_dbm + random.uniform(0, 8)\n",
    "\n",
    "    tx_power_dbm = random.uniform(10, 30)\n",
    "    channel_bw_mhz = random.choice(BANDWIDTHS_MHZ)\n",
    "    num_avail = random.randint(10, 50)\n",
    "    available_channels = sorted(random.sample(FREQS_MHZ, k=num_avail))\n",
    "\n",
    "    util_map = {(\"Urban\", \"Low\"): (20, 60), (\"Urban\", \"Medium\"): (40, 80), (\"Urban\", \"High\"): (60, 98),\n",
    "                (\"Rural\", \"Low\"): (0, 20), (\"Rural\", \"Medium\"): (10, 40), (\"Rural\", \"High\"): (20, 55)}\n",
    "    util_pct = random.uniform(*util_map[(environment, density)])\n",
    "    spectral_scan = {}\n",
    "    for ch in available_channels:\n",
    "        congestion_bump = np.random.normal(loc=util_pct / 100 * 8.0, scale=1.5)\n",
    "        spectral_scan[ch] = noise_floor_dbm + 2.0 + max(0.0, congestion_bump)\n",
    "\n",
    "    best = None\n",
    "    for ch in available_channels:\n",
    "        fspl = fspl_db(distance_m, ch)\n",
    "        loss = (weather_extra_loss_db(weather, distance_m)\n",
    "                + density_obstruction_factor(environment, density) * (distance_m / 1000.0)\n",
    "                + fresnel_penalty_db(fresnel_clear)\n",
    "                + obstruction_penalty_db(obstructed, obst_type))\n",
    "        rssi = (tx_power_dbm + TX_ANT_GAIN_DB + RX_ANT_GAIN_DB) - fspl - loss\n",
    "        interference_dbm = spectral_scan[ch]\n",
    "        snr = rssi - interference_dbm\n",
    "        score = snr - 0.25 * (interference_dbm - noise_floor_dbm)\n",
    "        if (best is None) or (score > best[3]):\n",
    "            best = (ch, rssi, snr, score)\n",
    "\n",
    "    ch_best, rssi_best, snr_best, _ = best\n",
    "    eff = snr_to_efficiency_bps_per_hz(max(-10.0, min(60.0, snr_best)))\n",
    "    expected_throughput_mbps = (eff * channel_bw_mhz * 1e6) / 1e6\n",
    "\n",
    "    return {\n",
    "        \"scenario_id\": scenario_id,\n",
    "        \"device_a_coordinates\": json.dumps([lat_a, lon_a, el_a]),\n",
    "        \"device_b_coordinates\": json.dumps([lat_b, lon_b, el_b]),\n",
    "        \"link_distance_m\": float(distance_m),\n",
    "        \"noise_dbm\": float(noise_dbm),\n",
    "        \"noise_floor_dbm\": float(noise_floor_dbm),\n",
    "        \"rssi_dbm\": float(rssi_best),\n",
    "        \"snr_db\": float(snr_best),\n",
    "        \"tx_power_dbm\": float(tx_power_dbm),\n",
    "        \"channel_bandwidth_mhz\": int(channel_bw_mhz),\n",
    "        \"channel_utilization_pct\": float(util_pct),\n",
    "        \"available_channels_mhz\": json.dumps(available_channels),\n",
    "        \"spectral_scan_dbm\": json.dumps(spectral_scan),\n",
    "        \"fresnel_clear_pct\": float(fresnel_clear),\n",
    "        \"weather_temp_c\": float(temp_c),\n",
    "        \"weather_humidity_pct\": float(humidity),\n",
    "        \"weather_condition\": weather,\n",
    "        \"image_obstruction_detected\": bool(obstructed),\n",
    "        \"image_obstruction_type\": obst_type,\n",
    "        \"environment_type\": environment,\n",
    "        \"area_density\": density,\n",
    "        \"recommended_channel_mhz\": int(ch_best),\n",
    "        \"expected_throughput_mbps\": float(expected_throughput_mbps),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset and save\n",
    "rows = [generate_synthetic_row(i) for i in range(N_SAMPLES)]\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(CSV_PATH, index=False)\n",
    "with zipfile.ZipFile(ZIP_PATH, 'w', compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    z.write(CSV_PATH, arcname=os.path.basename(CSV_PATH))\n",
    "\n",
    "def sha256_of(path, chunk=1024*1024):\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, 'rb') as f:\n",
    "        for b in iter(lambda: f.read(chunk), b''):\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "print(f\"Saved {len(df):,} rows -> {CSV_PATH}\")\n",
    "print('CSV SHA256:', sha256_of(CSV_PATH))\n",
    "print('ZIP SHA256:', sha256_of(ZIP_PATH))\n",
    "\n",
    "# Show head\n",
    "from IPython.display import display\n",
    "print(df.head(2).to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c9f52",
   "metadata": {},
   "source": [
    "## 4) Convert CSV -> conversational dataset (train/test)\n",
    "\n",
    "We adapt the original notebook's dataset loading to read the CSV we just generated and convert each row to a message pair `{role: user, content: prompt}, {role: assistant, content: target}`. The default target is `recommended_channel_mhz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "TARGET_FIELD = 'recommended_channel_mhz'  # change to expected_throughput_mbps if desired\n",
    "\n",
    "# Build prompt and target\n",
    "\n",
    "def build_prompt_from_row(r):\n",
    "    return (f\"Scenario id: {r['scenario_id']}\\n\"\n",
    "            f\"Distance (m): {r['link_distance_m']:.1f}\\n\"\n",
    "            f\"Env: {r['environment_type']} (density={r['area_density']})\\n\"\n",
    "            f\"Fresnel clear %: {r['fresnel_clear_pct']:.1f}\\n\"\n",
    "            f\"Weather: {r['weather_condition']}, temp C: {r['weather_temp_c']:.1f}\\n\"\n",
    "            f\"Noise floor (dBm): {r['noise_floor_dbm']:.1f}, RSSI (dBm): {r['rssi_dbm']:.1f}\\n\"\n",
    "            f\"Channel BW (MHz): {int(r['channel_bandwidth_mhz'])}\\n\\n\"\n",
    "            \"Question: Based on the scenario above, provide the best channel in MHz as a single integer (no explanation).\")\n",
    "\n",
    "def build_target_from_row(r):\n",
    "    return str(int(r[TARGET_FIELD])) if TARGET_FIELD == 'recommended_channel_mhz' else f\"{float(r[TARGET_FIELD]):.2f}\"\n",
    "\n",
    "# Convert to messages\n",
    "samples = []\n",
    "for _, row in df.iterrows():\n",
    "    prompt = build_prompt_from_row(row)\n",
    "    target = build_target_from_row(row)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": target}]\n",
    "    samples.append({\"messages\": messages})\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(samples))\n",
    "# train/test split\n",
    "if len(dataset) > 20:\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "else:\n",
    "    dataset = {'train': dataset}\n",
    "\n",
    "print('Dataset prepared. Train size:', len(dataset['train']))\n",
    "if 'test' in dataset:\n",
    "    print('Test size:', len(dataset['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df07e5",
   "metadata": {},
   "source": [
    "## 5) (Optional) Mount Drive / Hugging Face login\n",
    "\n",
    "If you're running in Colab, mount your Google Drive to save checkpoints and/or store your HF token in Colab userdata. Otherwise set `hf_token` as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab-specific helpers (uncomment when using Colab)\n",
    "# from google.colab import drive, userdata\n",
    "# drive.mount('/content/drive')\n",
    "# hf_token = userdata.get('HF_TOKEN')\n",
    "# from huggingface_hub import login\n",
    "# login(hf_token)\n",
    "\n",
    "print('If running in Colab, mount drive and login to Hugging Face as needed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d9667",
   "metadata": {},
   "source": [
    "## 6) Load model & tokenizer (Gemma base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee008b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Base model (change as needed)\n",
    "base_model = 'google/gemma-3-270m-it'\n",
    "checkpoint_dir = os.path.join(OUT_DIR, 'gemma_finetune_checkpoint')\n",
    "\n",
    "print('Loading model (this may require accepting the model license on HF)')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto',\n",
    "    attn_implementation='eager',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "print('Model device:', model.device)\n",
    "print('Model dtype:', model.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a24b42",
   "metadata": {},
   "source": [
    "## 7) Configure SFT and trainer (TRL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "torch_dtype = model.dtype\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=checkpoint_dir,\n",
    "    max_length=512,\n",
    "    packing=False,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_checkpointing=False,\n",
    "    optim='adamw_torch_fused',\n",
    "    logging_steps=1,\n",
    "    save_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True if torch_dtype == torch.float16 else False,\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,\n",
    "    lr_scheduler_type='constant',\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    dataset_kwargs={\n",
    "        'add_special_tokens': False,\n",
    "        'append_concat_token': True,\n",
    "    }\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'] if 'test' in dataset else None,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print('Trainer created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08c430",
   "metadata": {},
   "source": [
    "## 8) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training (warning: this will use GPU and can be long)\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "print('Training finished; model saved to', checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f650846",
   "metadata": {},
   "source": [
    "## 9) Test model inference on test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "if 'test' in dataset:\n",
    "    for i in range(min(5, len(dataset['test']))):\n",
    "        sample = dataset['test'][i]\n",
    "        prompt = pipe.tokenizer.apply_chat_template(sample['messages'][:1], tokenize=False, add_generation_prompt=True)\n",
    "        outputs = pipe(prompt, max_new_tokens=64, disable_compile=True)\n",
    "        print('Question:')\n",
    "        print(sample['messages'][0]['content'])\n",
    "        print('Original Answer:')\n",
    "        print(sample['messages'][1]['content'])\n",
    "        print('Generated Answer:')\n",
    "        print(outputs[0]['generated_text'][len(prompt):].strip())\n",
    "        print('-'*60)\n",
    "else:\n",
    "    print('No test split available to run inference on.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5dbb75",
   "metadata": {},
   "source": [
    "## 10) Save artifacts and simple loader example\n",
    "\n",
    "Demonstrates saving the generated CSV, the zipped archive, and how you could reload them for offline use. The trainer already saved model checkpoints to `checkpoint_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load CSV back\n",
    "reloaded = pd.read_csv(CSV_PATH)\n",
    "print('Reloaded rows:', len(reloaded))\n",
    "print(reloaded.columns[:10])\n",
    "\n",
    "# Example: load a saved model (path: checkpoint_dir)\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint_dir)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "\n",
    "print('Notebook completed. Increase N_SAMPLES and epochs for full training runs.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
