{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8fd8cfc",
   "metadata": {},
   "source": [
    "# Annotated LoRA run (walkthrough)\n",
    "\n",
    "This short notebook documents a small LoRA smoke run for the project and includes annotated steps, commands, and placeholder locations for screenshots you may capture while running in Colab. It's written so a beginner can follow along. If you open this in Colab, uncomment the install cell and run; otherwise run the shell commands locally as shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc99b2",
   "metadata": {},
   "source": [
    "## 1) Install lightweight deps (Colab)\n",
    "\n",
    "Uncomment and run the cell below in Colab to install minimal dependencies for a LoRA smoke run. In a local dev environment you can instead run `pip install -r requirements.txt` and `pip install peft` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6799ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Colab, uncomment and run this to install peft (LoRA) and any light deps\n",
    "# %pip install -q peft transformers datasets pandas\n",
    "print('If running locally, ensure requirements.txt + peft are installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc07a3",
   "metadata": {},
   "source": [
    "## 2) Prepare a tiny dataset (smoke)\n",
    "We use the repository's smoke generator to create a small CSV you can use to validate the training pipeline without long runs or GPUs. Run this locally or in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ac0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the smoke generator to produce a small CSV\n",
    "!python scripts/generate_synthetic_smoke.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b750945",
   "metadata": {},
   "source": [
    "## 3) Run a LoRA smoke command (dry run first)\n",
    "First try `--dry-run` to validate dataset and trainer construction won't fail. Then run with `--mode lora` for a real adapter attach and brief epoch.\n",
    "\n",
    "Command (dry-run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3513160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dry-run: prepares dataset and skips heavy imports/training\n",
    "!python scripts/finetune_gemma_from_csv.py --csv synthetic_wifi_5ghz_outdoor_smoke.csv --dry-run --max-rows 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b8b740",
   "metadata": {},
   "source": [
    "If the dry-run prints a dataset summary and exits, it's safe to proceed.\n",
    "\n",
    "Run the real LoRA smoke (small epoch) below; this will attach LoRA adapters and run training (requires GPU for speed but can run on CPU for small examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc704c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA smoke training (small)\n",
    "# Increase --num-epochs for real experiments, remove --max-rows then\n",
    "!python scripts/finetune_gemma_from_csv.py --csv synthetic_wifi_5ghz_outdoor_smoke.csv --mode lora --lora-r 8 --lora-alpha 32 --lora-dropout 0.05 --num-epochs 1 --per-device-batch-size 2 --max-rows 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf9abc",
   "metadata": {},
   "source": [
    "## 4) Annotate screenshots (placeholders)\n",
    "If running in Colab, capture screenshots for: model load memory usage, trainer creation, and a final checkpoint saved. Place them in `site/en/gemma/docs/core/screenshots/` and reference them here: \n",
    "\n",
    "![model-load](/site/en/gemma/docs/core/screenshots/model_load.png)\n",
    "![trainer-created](/site/en/gemma/docs/core/screenshots/trainer_created.png)\n",
    "![checkpoint-saved](/site/en/gemma/docs/core/screenshots/checkpoint_saved.png)\n",
    "\n",
    "Replace those files with real screenshots when available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9594a02",
   "metadata": {},
   "source": [
    "## 5) Troubleshooting notes\n",
    "- If you run out of memory, reduce `--per-device-batch-size` and try `--mode lora`.\n",
    "- If LoRA adapter attach fails, ensure `peft` is installed and compatible with your `transformers` version.\n",
    "- For QLoRA, ensure `bitsandbytes` and `peft` are installed in a CUDA-enabled runtime."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}