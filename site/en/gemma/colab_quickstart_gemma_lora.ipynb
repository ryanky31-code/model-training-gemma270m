{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d7cab02",
   "metadata": {},
   "source": [
    "# Gemma-3 270M LoRA / QLoRA Quickstart (Colab)\n",
    "\n",
    "This notebook helps you set up a Colab GPU runtime, install required packages, provide your Hugging Face token, choose a model variant, and run a small dry-run of the project's training CLI in `--mode lora` or `--mode qlora`.\n",
    "\n",
    "Open the runtime type as `GPU` before running the install cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d7329",
   "metadata": {},
   "source": [
    "## 1) Install dependencies (run in a GPU runtime)\n",
    "Run the cell below to install the core dependencies. For QLoRA you also need a compatible CUDA + bitsandbytes build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies. This may take a few minutes in Colab.\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers accelerate datasets safetensors\n",
    "!pip install -r requirements-dev.txt\n",
    "# bitsandbytes and peft are optional but useful for LoRA/QLoRA runs; the pip may fail on some envs, so allow failure\n",
    "!pip install git+https://github.com/huggingface/peft.git || true\n",
    "!pip install bitsandbytes || true\n",
    "print('Install step finished (errors for bitsandbytes may be expected on some Colab CUDA versions).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee94e0",
   "metadata": {},
   "source": [
    "## 2) Provide your Hugging Face token\n",
    "Run the cell and paste your HF token when prompted. The token is stored in the environment for the notebook session only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ced466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "token = getpass('Enter your Hugging Face token (input hidden): ')\n",
    "if token:\n",
    "  os.environ['HF_TOKEN'] = token\n",
    "  print('HF token set in environment for this session')\n",
    "else:\n",
    "  print('No token provided; some actions may be skipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad96a7",
   "metadata": {},
   "source": [
    "## 3) Choose model variant and mode\n",
    "Use the next cell to pick either `google/gemma-3-270m` (default) or another compatible Gemma checkpoint. Toggle QLoRA if you want to attempt 4-bit quantized flow (only if bitsandbytes is installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model id (change if you have a different HF checkpoint)\n",
    "model_id = 'google/gemma-3-270m'\n",
    "# Set to True to attempt QLoRA (requires bitsandbytes + CUDA)\n",
    "markdown\n",
    "markdown\n",
    "## 10) Push trained model to Hugging Face Hub\n",
    "This cell will push a local model directory to the Hub. Ensure `HF_TOKEN` is set in the session and you have `huggingface_hub` installed. Edit `HUB_REPO_ID` to a repo you control (e.g., 'your-username/gemma3-finetuned').\n",
    "code\n",
    "python\n",
    "# Push model to HF Hub (requires HF_TOKEN and huggingface_hub installed)\n",
    "import os\n",
    "from pathlib import Path\n",
    "HUB_REPO_ID = 'your-username/gemma3-finetuned'  # EDIT_THIS\n",
    "MODEL_DIR = 'outputs/gemma3-lora-run/best_checkpoint'  # set to your final model dir\n",
    "\n",
    "if not os.environ.get('HF_TOKEN'):\n",
    "    print('HF_TOKEN not set in environment; set it and re-run this cell')\n",
    "else:\n",
    "    try:\n",
    "        from huggingface_hub import HfApi, Repository\n",
    "        api = HfApi()\n",
    "        print('Creating or ensuring repo exists...')\n",
    "        api.create_repo(HUB_REPO_ID, private=False, exist_ok=True, token=os.environ['HF_TOKEN'])\n",
    "        repo_local = Path('hf_repo')\n",
    "        if not repo_local.exists():\n",
    "            print('Cloning remote repo...')\n",
    "            repo = Repository(local_dir=str(repo_local), clone_from=HUB_REPO_ID, use_auth_token=os.environ['HF_TOKEN'])\n",
    "        else:\n",
    "            from huggingface_hub import Repository as _Repo\n",
    "            repo = _Repo(local_dir=str(repo_local))\n",
    "        print('Copying model files...')\n",
    "        import shutil\n",
    "        shutil.rmtree(repo_local, ignore_errors=True)\n",
    "        repo_local.mkdir(parents=True, exist_ok=True)\n",
    "        # copy model checkpoint dir into repo_local\n",
    "        shutil.copytree(MODEL_DIR, repo_local / Path(MODEL_DIR).name)\n",
    "        print('Pushing to Hub... this may take a while')\n",
    "        repo.push_to_hub(commit_message='Add fine-tuned Gemma adapter', blocking=True)\n",
    "        print('Pushed to', HUB_REPO_ID)\n",
    "    except Exception as e:\n",
    "        print('Failed to push to Hub:', e)\n",
    "use_qlora = False\n",
    "# Example overrides: model_id = 'google/gemma-3-270m'\n",
    "print(f'Model: {model_id}, QLoRA enabled: {use_qlora}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f934a",
   "metadata": {},
   "source": [
    "## 4) Quick check: CUDA / wheel helper\n",
    "This cell inspects the runtime CUDA version (if available) and prints recommended pip commands for installing PyTorch and bitsandbytes suitable for common Colab CUDA versions. Use the printed command to (re)install if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c661ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, re\n",
    "def get_nvidia_smi():\n",
    "    try:\n",
    "        out = subprocess.check_output(['nvidia-smi','--query-gpu=driver_version,cuda_version','--format=csv,noheader'])\n",
    "        return out.decode().strip()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "info = get_nvidia_smi()\n",
    "print('nvidia-smi info:', info)\n",
    "cuda_tag = 'cpu'\n",
    "if info:\n",
    "    m = re.search(r'([0-9]+[0-9]+)', info)\n",
    "    if m:\n",
    "        v = m.group(1)\n",
    "        # Map common Colab CUDA to torch wheel tags (best-effort)\n",
    "        if v.startswith('12.1') or v.startswith('12.0'):\n",
    "            cuda_tag = 'cu121'\n",
    "        elif v.startswith('11.8'):\n",
    "            cuda_tag = 'cu118'\n",
    "        elif v.startswith('11.7'):\n",
    "            cuda_tag = 'cu117'\n",
    "        else:\n",
    "            cuda_tag = 'cu118'\n",
    "print('Detected CUDA tag suggestion:', cuda_tag)\n",
    "if cuda_tag != 'cpu':\n",
    "    print('Recommended pip install (example):')\n",
    "    print(f"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
