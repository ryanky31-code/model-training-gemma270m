{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ryanky31-code/model-training-gemma270m/blob/main/site/en/gemma/colab_quickstart_gemma_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7cab02",
   "metadata": {},
   "source": [
    "# Gemma-3 270M LoRA / QLoRA Quickstart (Colab)\n",
    "\n",
    "This notebook helps you set up a Colab GPU runtime, install required packages, provide your Hugging Face token, choose a model variant, and run a small dry-run of the project's training CLI in `--mode lora` or `--mode qlora`.\n",
    "\n",
    "Open the runtime type as `GPU` before running the install cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d7329",
   "metadata": {},
   "source": [
    "## 1) Install dependencies (run in a GPU runtime)\n",
    "Run the cell below to install the core dependencies. For QLoRA you also need a compatible CUDA + bitsandbytes build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies. This may take a few minutes in Colab.\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers accelerate datasets safetensors\n",
    "!pip install -r requirements-dev.txt\n",
    "# bitsandbytes and peft are optional but useful for LoRA/QLoRA runs; the pip may fail on some envs, so allow failure\n",
    "!pip install git+https://github.com/huggingface/peft.git || true\n",
    "!pip install bitsandbytes || true\n",
    "print('Install step finished (errors for bitsandbytes may be expected on some Colab CUDA versions).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee94e0",
   "metadata": {},
   "source": [
    "## 2) Provide your Hugging Face token\n",
    "Run the cell and paste your HF token when prompted. The token is stored in the environment for the notebook session only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ced466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "token = getpass('Enter your Hugging Face token (input hidden): ')\n",
    "if token:\n",
    "  os.environ['HF_TOKEN'] = token\n",
    "  print('HF token set in environment for this session')\n",
    "else:\n",
    "  print('No token provided; some actions may be skipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad96a7",
   "metadata": {},
   "source": [
    "## 3) Choose model variant and mode\n",
    "Use the next cell to pick either `google/gemma-3-270m` (default) or another compatible Gemma checkpoint. Toggle QLoRA if you want to attempt 4-bit quantized flow (only if bitsandbytes is installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model id (change if you have a different HF checkpoint)\n",
    "model_id = 'google/gemma-3-270m'\n",
    "# Set to True to attempt QLoRA (requires bitsandbytes + CUDA)\n",
    "use_qlora = False\n",
    "# Example overrides: model_id = 'google/gemma-3-270m'\n",
    "print(f'Model: {model_id}, QLoRA enabled: {use_qlora}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f934a",
   "metadata": {},
   "source": [
    "## 4) Quick check: CUDA / wheel helper\n",
    "This cell inspects the runtime CUDA version (if available) and prints recommended pip commands for installing PyTorch and bitsandbytes suitable for common Colab CUDA versions. Use the printed command to (re)install if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c661ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, re\n",
    "def get_nvidia_smi():\n",
    "    try:\n",
    "        out = subprocess.check_output(['nvidia-smi','--query-gpu=driver_version,cuda_version','--format=csv,noheader'])\n",
    "        return out.decode().strip()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "info = get_nvidia_smi()\n",
    "print('nvidia-smi info:', info)\n",
    "cuda_tag = 'cpu'\n",
    "if info:\n",
    "    m = re.search(r'([0-9]+[0-9]+)', info)\n",
    "    if m:\n",
    "        v = m.group(1)\n",
    "        # Map common Colab CUDA to torch wheel tags (best-effort)\n",
    "        if v.startswith('12.1') or v.startswith('12.0'):\n",
    "            cuda_tag = 'cu121'\n",
    "        elif v.startswith('11.8'):\n",
    "            cuda_tag = 'cu118'\n",
    "        elif v.startswith('11.7'):\n",
    "            cuda_tag = 'cu117'\n",
    "        else:\n",
    "            cuda_tag = 'cu118'\n",
    "print('Detected CUDA tag suggestion:', cuda_tag)\n",
    "if cuda_tag != 'cpu':\n",
    "    print('Recommended pip install (example):')\n",
    "    print(f"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
