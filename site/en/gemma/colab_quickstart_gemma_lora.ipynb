{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d7cab02",
   "metadata": {},
   "source": [
    "# Gemma-3 270M LoRA / QLoRA Quickstart (Colab)\n",
    "\n",
    "This notebook helps you set up a Colab GPU runtime, install required packages, provide your Hugging Face token, choose a model variant, and run a small dry-run of the project's training CLI in `--mode lora` or `--mode qlora`.\n",
    "\n",
    "Open the runtime type as `GPU` before running the install cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d7329",
   "metadata": {},
   "source": [
    "## 1) Install dependencies (run in a GPU runtime)\n",
    "Run the cell below to install the core dependencies. For QLoRA you also need a compatible CUDA + bitsandbytes build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies. This may take a few minutes in Colab.\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers accelerate datasets safetensors\n",
    "!pip install -r requirements-dev.txt\n",
    "# bitsandbytes and peft are optional but useful for LoRA/QLoRA runs; the pip may fail on some envs, so allow failure\n",
    "!pip install git+https://github.com/huggingface/peft.git || true\n",
    "!pip install bitsandbytes || true\n",
    "print('Install step finished (errors for bitsandbytes may be expected on some Colab CUDA versions).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee94e0",
   "metadata": {},
   "source": [
    "## 2) Provide your Hugging Face token\n",
    "Run the cell and paste your HF token when prompted. The token is stored in the environment for the notebook session only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ced466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "token = getpass('Enter your Hugging Face token (input hidden): ')\n",
    "if token:\n",
    "  os.environ['HF_TOKEN'] = token\n",
    "  print('HF token set in environment for this session')\n",
    "else:\n",
    "  print('No token provided; some actions may be skipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad96a7",
   "metadata": {},
   "source": [
    "## 3) Choose model variant and mode\n",
    "Use the next cell to pick either `google/gemma-3-270m` (default) or another compatible Gemma checkpoint. Toggle QLoRA if you want to attempt 4-bit quantized flow (only if bitsandbytes is installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model id (change if you have a different HF checkpoint)\n",
    "model_id = 'google/gemma-3-270m'\n",
    "# Set to True to attempt QLoRA (requires bitsandbytes + CUDA)\n",
    "use_qlora = False\n",
    "# Example overrides: model_id = 'google/gemma-3-270m'\n",
    "print(f'Model: {model_id}, QLoRA enabled: {use_qlora}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f934a",
   "metadata": {},
   "source": [
    "## 4) Quick check: CUDA / wheel helper\n",
    "This cell inspects the runtime CUDA version (if available) and prints recommended pip commands for installing PyTorch and bitsandbytes suitable for common Colab CUDA versions. Use the printed command to (re)install if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c661ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, re\n",
    "def get_nvidia_smi():\n",
    "    try:\n",
    "        out = subprocess.check_output(['nvidia-smi','--query-gpu=driver_version,cuda_version','--format=csv,noheader'])\n",
    "        return out.decode().strip()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "info = get_nvidia_smi()\n",
    "print('nvidia-smi info:', info)\n",
    "cuda_tag = 'cpu'\n",
    "if info:\n",
    "    m = re.search(r'([0-9]+[0-9]+)', info)\n",
    "    if m:\n",
    "        v = m.group(1)\n",
    "        # Map common Colab CUDA to torch wheel tags (best-effort)\n",
    "        if v.startswith('12.1') or v.startswith('12.0'):\n",
    "            cuda_tag = 'cu121'\n",
    "        elif v.startswith('11.8'):\n",
    "            cuda_tag = 'cu118'\n",
    "        elif v.startswith('11.7'):\n",
    "            cuda_tag = 'cu117'\n",
    "        else:\n",
    "            cuda_tag = 'cu118'\n",
    "print('Detected CUDA tag suggestion:', cuda_tag)\n",
    "if cuda_tag != 'cpu':\n",
    "    print('Recommended pip install (example):')\n",
    "    print(f'pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/{cuda_tag}')\n",
    "    print(f'pip install bitsandbytes')\n",
    "else:\n",
    "    print('CPU-only mode detected (no CUDA), LoRA may still work but will be slower')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d8a9b",
   "metadata": {},
   "source": [
    "## 5) Run a LoRA/QLoRA training smoke test\n",
    "This cell runs a minimal training test using the project's CLI. Adjust `--max_train_steps` and `--dataset_name` as needed for a quick validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a small LoRA training test\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Set mode based on the use_qlora flag\n",
    "mode = 'qlora' if use_qlora else 'lora'\n",
    "print(f'Running training in {mode} mode...')\n",
    "\n",
    "# Basic training command - adjust parameters as needed\n",
    "cmd = [\n",
    "    'python', 'scripts/finetune_gemma_from_csv.py',\n",
    "    '--csv', 'synthetic_wifi_5ghz_outdoor_smoke.csv',\n",
    "    '--mode', mode,\n",
    "    '--base-model', model_id,\n",
    "    '--mode', mode,\n",
    "    '--num-epochs', '1',  # Very short for smoke test\n",
    "    '--max-rows', '100',  # Limit rows for quick test\n",
    "    '--checkpoint-dir', 'outputs/colab-smoke-test'\n",
    "]\n",
    "\n",
    "# Run the training\n",
    "try:\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "    print('STDOUT:')\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print('STDERR:')\n",
    "        print(result.stderr)\n",
    "    print(f'Training finished with return code: {result.returncode}')\n",
    "except subprocess.TimeoutExpired:\n",
    "    print('Training timed out after 5 minutes - this is expected for a smoke test')\n",
    "except Exception as e:\n",
    "    print(f'Error running training: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7f5a1e",
   "metadata": {},
   "source": [
    "## 6) Check training results\n",
    "This cell lists the output directory to see what files were created during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f3c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path('outputs/colab-smoke-test')\n",
    "if output_dir.exists():\n",
    "    print(f'Contents of {output_dir}:')\n",
    "    for item in output_dir.rglob('*'):\n",
    "        if item.is_file():\n",
    "            print(f'  {item.relative_to(output_dir)} ({item.stat().st_size} bytes)')\n",
    "else:\n",
    "    print(f'Output directory {output_dir} does not exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8f4c1a",
   "metadata": {},
   "source": [
    "## 7) Push trained model to Hugging Face Hub (Optional)\n",
    "This cell will push a local model directory to the Hub. Ensure `HF_TOKEN` is set in the session and you have `huggingface_hub` installed. Edit `HUB_REPO_ID` to a repo you control (e.g., 'your-username/gemma3-finetuned')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f3a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push model to HF Hub (requires HF_TOKEN and huggingface_hub installed)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "HUB_REPO_ID = 'your-username/gemma3-finetuned'  # EDIT_THIS\n",
    "MODEL_DIR = 'outputs/colab-smoke-test'  # set to your final model dir\n",
    "\n",
    "if not os.environ.get('HF_TOKEN'):\n",
    "    print('HF_TOKEN not set in environment; set it and re-run this cell')\n",
    "elif not Path(MODEL_DIR).exists():\n",
    "    print(f'Model directory {MODEL_DIR} does not exist; train a model first')\n",
    "else:\n",
    "    try:\n",
    "        # Install huggingface_hub if needed\n",
    "        try:\n",
    "            from huggingface_hub import HfApi\n",
    "        except ImportError:\n",
    "            import subprocess\n",
    "            subprocess.check_call(['pip', 'install', 'huggingface_hub'])\n",
    "            from huggingface_hub import HfApi\n",
    "        \n",
    "        api = HfApi()\n",
    "        print('Creating or ensuring repo exists...')\n",
    "        api.create_repo(HUB_REPO_ID, private=False, exist_ok=True, token=os.environ['HF_TOKEN'])\n",
    "        \n",
    "        print('Uploading model files...')\n",
    "        api.upload_folder(\n",
    "            folder_path=MODEL_DIR,\n",
    "            repo_id=HUB_REPO_ID,\n",
    "            token=os.environ['HF_TOKEN'],\n",
    "            commit_message='Add fine-tuned Gemma adapter'\n",
    "        )\n",
    "        print(f'Successfully pushed model to {HUB_REPO_ID}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to push to Hub: {e}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}